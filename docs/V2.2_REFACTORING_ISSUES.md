# V2.2 Refactoring Issues - Critical Analysis

**Date:** 2026-01-13
**Status:** CRITICAL - Multiple regressions discovered
**Recommendation:** Revert to pre-v2.2 or comprehensive fix required

---

## Executive Summary

The v2.2 refactoring introduced the EventStore observability framework but left multiple critical systems broken or incomplete. While the refactoring improved code organization and added file-based logging, it broke:

1. **File preview generation** (LLM column hallucination)
2. **Database observability** (8/10 tables empty)
3. **Column metadata persistence** (enrichment lost)

**Impact:** Production-breaking. Data loads successfully but metadata is lost, observability is blind, and LLM enrichment produces incorrect results.

---

## Issue #1: Preview Generation Deleted

### What Worked (Pre-v2.2)

**File:** `scripts/url_to_manifest.py` (commit `b4bf5ee^`)

```python
def process_resources(resources, source_url, enable_preview=False):
    processed = []

    # Process files...

    if enable_preview:
        print("üîç Generating content previews...")
        for item in processed:
            # Download file
            tmp_path = download_file(item['url'])

            # Read actual columns
            if item['type'] == 'CSV':
                df = pd.read_csv(tmp_path, nrows=10)
                item['preview'] = format_preview(df)
            elif item['type'] == 'XLSX':
                # Use FileExtractor to read sheet columns
                extractor = FileExtractor(tmp_path)
                item['preview'] = extractor.get_preview(sheet)
```

**Result:** Manifest contained actual column names from files.

### What Broke (v2.2)

**File:** `src/datawarp/pipeline/manifest.py` (commit `53e4988`)

```python
def generate_manifest(url, output_path, skip_preview=False):
    # Scrape URLs
    resources = scrape_resources(url)

    # Process files
    processed = process_resources(resources, url)

    # Generate sources
    sources = generate_sources(processed)

    # Write YAML
    # ‚ùå PREVIEW GENERATION CODE DELETED
    # ‚ùå skip_preview parameter exists but does nothing
```

**Result:** Manifests have NO column information. LLM invents columns.

### Evidence

**Enriched manifest said:**
```yaml
columns:
  - name: Organisation Code
  - name: Organisation Name
  - name: Number of Referrals
  - name: Number of Assessments
  - name: Number of Diagnoses
```

**Actual CSV contained:**
```
REPORTING_PERIOD_START_DATE, REPORTING_PERIOD_END_DATE, INDICATOR_ID, AGE_GROUP, VALUE
```

**Only 0/11 columns matched.** Complete hallucination.

### Fix Applied (2026-01-13)

Added `add_file_preview()` function back to `manifest.py` (commit `de77cf9`):

```python
def add_file_preview(file_entry: dict, event_store=None):
    """Download file and extract actual columns."""
    url = file_entry['url']
    response = requests.get(url)

    with tempfile.NamedTemporaryFile() as tmp:
        tmp.write(response.content)

        if file.endswith('.csv'):
            df = pd.read_csv(tmp.name, nrows=3)
            file_entry['preview'] = {
                'columns': df.columns.tolist(),
                'sample_rows': df.head(3).to_dict('records')
            }
```

**Status:** ‚úÖ FIXED (commit de77cf9)

---

## Issue #2: Database Observability Abandoned

### What Worked (Pre-v2.2)

**File:** `src/datawarp/loader/batch.py`

```python
def load_batch(manifest_path: Path):
    manifest_data = yaml.safe_load(manifest_path.read_text())

    for source in manifest_data['sources']:
        for file in source['files']:
            # Load data
            result = load_file(file['url'], source['code'])

            # ‚úÖ Track in database
            repository.record_manifest_file(
                manifest_name=manifest_data['name'],
                source_code=source['code'],
                file_url=file['url'],
                status='success',
                rows_loaded=result.rows_loaded,
                columns_added=result.columns_added
            )
```

**Result:** `tbl_manifest_files` populated with load history.

### What Broke (v2.2)

**File:** `scripts/backfill.py`

```python
def process_period(pub_code, pub_config, period, url, event_store):
    # Generate manifest
    # Enrich manifest
    # Load to database

    for source in manifest['sources']:
        result = load_file(file['url'], source['code'])

        if result.success:
            sources_loaded += 1

            # ‚úÖ Log to EventStore (files only)
            event_store.emit(create_event(
                EventType.STAGE_COMPLETED,
                message=f"Loaded {result.rows_loaded} rows"
            ))

            # ‚ùå NO database tracking
            # ‚ùå repository.record_manifest_file() NOT CALLED
```

**Result:** EventStore writes to `.log` and `.jsonl` files, but **database tables stay empty**.

### Dead Tables

| Table | Purpose | Status | Evidence |
|-------|---------|--------|----------|
| `tbl_enrichment_runs` | Track LLM enrichment calls | ‚ùå 0 rows | Schema exists, never populated |
| `tbl_enrichment_api_calls` | LLM API metrics (tokens, cost) | ‚ùå 0 rows | Schema exists, never populated |
| `tbl_manifest_files` | Per-file load tracking | ‚ùå 0 rows | Schema exists, never populated |
| `tbl_pipeline_log` | Pipeline event log | ‚ùå 0 rows | Schema exists, never populated |
| `tbl_source_mappings` | LLM code ‚Üí canonical mapping | ‚ùå 0 rows | Phase 1 feature, never used |
| `tbl_canonical_sources` | Canonical source registry | ‚ùå 0 rows | Phase 1 feature, never used |
| `tbl_drift_events` | Schema change tracking | ‚ùå 0 rows | Phase 1 feature, never used |
| `tbl_column_metadata` | Column descriptions | ‚ùå 0 rows | Fixed 2026-01-13 (commit b5785da) |

**8 out of 10 observability tables are dead code.**

### What EventStore Actually Does

**File:** `src/datawarp/supervisor/events.py`

```python
class EventStore:
    def emit(self, event: Event):
        """Emit event to all outputs."""
        self._emit_console(event)    # ‚úÖ Print to stdout
        self._emit_file_log(event)   # ‚úÖ Write to logs/backfill_*.log
        self._emit_jsonl(event)      # ‚úÖ Write to logs/events/*.jsonl
        # self._emit_database(event) # ‚ùå DOES NOT EXIST
```

**Only file-based logging works. Database sink never implemented.**

### Repository Functions That Exist But Aren't Called

```python
# src/datawarp/storage/repository.py

def record_manifest_file(...):           # ‚ùå Never called
def check_manifest_file_status(...):     # ‚ùå Never called
def get_manifest_summary(...):          # ‚ùå Never called
def get_manifest_errors(...):           # ‚ùå Never called
def get_manifest_files(...):            # ‚ùå Never called
```

These functions exist, are fully implemented, but **backfill.py never calls them**.

### Status

‚ùå **NOT FIXED** - Database observability completely broken.

---

## Issue #3: Column Metadata Not Persisted

### What Should Happen

```
1. Generate manifest ‚Üí CSV/Excel files (no columns yet)
2. Add preview ‚Üí Download files, extract columns
3. LLM enrichment ‚Üí Add descriptions, semantic names
4. Load data ‚Üí Store to PostgreSQL
5. Store metadata ‚Üí Persist enriched columns to tbl_column_metadata
6. Export Parquet ‚Üí Include enriched descriptions in .md file
```

### What Actually Happened (v2.2)

```
1. Generate manifest ‚Üí ‚úÖ Works
2. Add preview ‚Üí ‚ùå BROKEN (Issue #1)
3. LLM enrichment ‚Üí ‚ùå Hallucinates columns (no preview data)
4. Load data ‚Üí ‚úÖ Works (uses actual columns from Extractor)
5. Store metadata ‚Üí ‚ùå NOT IMPLEMENTED
6. Export Parquet ‚Üí ‚ùå Generates .md without descriptions
```

### Evidence

**Enriched manifest has rich metadata:**
```yaml
columns:
  - name: REPORTING_PERIOD_START_DATE
    semantic_name: reporting_period_start_date
    description: The start date of the reporting period for the data.
  - name: INDICATOR_ID
    semantic_name: indicator_id
    description: A unique identifier for the specific ADHD indicator.
```

**Database has NO metadata:**
```sql
SELECT COUNT(*) FROM datawarp.tbl_column_metadata;
-- Result: 0
```

**Parquet .md export is useless:**
```markdown
## Columns
- `reporting_period_start_date` (character varying)
- `indicator_id` (character varying)
```

No descriptions, no semantic names, no connection to enriched manifest.

### Fix Applied (2026-01-13)

**1. Store metadata during load** (commit `b5785da`):

```python
# scripts/backfill.py
if result.success:
    sources_loaded += 1

    # NEW: Store column metadata
    if source.get('columns'):
        repository.store_column_metadata(
            canonical_source_code=source_code,
            columns=source['columns'],
            conn=conn
        )
```

**2. Export enriched metadata** (commit `b5785da`):

```python
# src/datawarp/pipeline/exporter.py
cur.execute("""
    SELECT c.column_name, c.data_type, cm.description, cm.original_name
    FROM information_schema.columns c
    LEFT JOIN datawarp.tbl_column_metadata cm
        ON cm.canonical_source_code = %s
        AND LOWER(cm.column_name) = LOWER(c.column_name)
    WHERE c.table_schema || '.' || c.table_name = %s
""", (canonical_code, table_name))

# Generate rich .md with descriptions
for col in columns:
    if col['description']:
        md += f"### `{col['name']}`\n"
        md += f"**Type:** `{col['type']}`\n\n"
        md += f"{col['description']}\n\n"
```

**Status:** ‚úÖ FIXED (commit b5785da) - But only tested with fresh data, not verified end-to-end

---

## Issue #4: Canonicalization Works But Inconsistently

### Background

Phase 1 canonicalization was implemented (commit `b5ddf8e`) to strip date patterns from codes:

```
adhd_may25_data ‚Üí adhd_data
pcn_workforce_apr2024 ‚Üí pcn_workforce
```

This enables cross-period consolidation (May + November ‚Üí same table).

### What Works

**Canonicalization module exists:**
```python
# src/datawarp/pipeline/canonicalize.py
def remove_date_patterns(code: str) -> str:
    """Strip month names, years, quarters."""
    code = re.sub(r'_(jan|feb|mar|...|dec)\d{2,4}', '', code)
    code = re.sub(r'_20\d{2}', '', code)
    # etc.
    return code
```

**Backfill integrates it:**
```python
# scripts/backfill.py
# After enrichment, before loading
canonical_manifest = canonicalize_manifest(enriched_manifest)
```

### What's Inconsistent

**Reference matching never tested in production.** The workflow is:

```
Period 1 (May):  Enrich without reference ‚Üí adhd_may25_data ‚Üí Canonicalize ‚Üí adhd_data
Period 2 (Nov):  Enrich WITH reference (May) ‚Üí Match by URL ‚Üí Reuse adhd_data code
```

But `config/publications.yaml` has no `reference_manifest` field, so Period 2 would generate new codes instead of reusing.

### Status

‚ö†Ô∏è **PARTIAL** - Canonicalization works for first period, but reference-based matching for subsequent periods is untested.

---

## Root Cause Analysis

### Why v2.2 Refactoring Failed

**1. Incomplete Implementation**
- EventStore designed but database sink never added
- Preview generation removed but not replaced
- Column metadata storage designed but not wired up

**2. No Integration Testing**
- Each module works in isolation
- End-to-end flow broken
- No validation that data flows through full pipeline

**3. Breaking Changes Without Verification**
- Old `batch.py` replaced with `backfill.py`
- Assumed new code maintained feature parity
- Did not verify database tables populated

**4. Schema-Code Mismatch**
- SQL schema creates 10 observability tables
- Python code only populates 2
- Dead code accumulated

### What Worked Well

- ‚úÖ EventStore console output (clean, structured)
- ‚úÖ File-based logging (.log, .jsonl)
- ‚úÖ Code organization (pipeline library)
- ‚úÖ Modular architecture
- ‚úÖ Type hints and documentation

### What Failed

- ‚ùå Database observability
- ‚ùå Preview generation
- ‚ùå Column metadata flow
- ‚ùå Integration testing
- ‚ùå Feature parity validation

---

## Commits That Broke Things

| Commit | Description | What Broke |
|--------|-------------|------------|
| `b4bf5ee` | EventStore + pipeline library refactoring (v2.2) | Preview generation deleted |
| `53e4988` | Complete v2.2 refactoring | Database observability abandoned |
| `d68e7d8` | Session 17 - Operational observability | Created tables but no population logic |

---

## Commits That Fixed Things

| Commit | Description | What Fixed |
|--------|-------------|------------|
| `cb9819d` | Add canonicalization step | Date pattern removal from codes |
| `1115de6` | Fix reset_db.py | Added missing Phase 1 tables |
| `de77cf9` | Restore file preview generation | LLM sees actual columns now |
| `b5785da` | Store and export enriched metadata | Column descriptions persist |

---

## Current State (2026-01-13)

### What Works ‚úÖ

1. **Data loading** - Files download and load to PostgreSQL correctly
2. **Schema evolution** - ALTER TABLE ADD COLUMN on drift
3. **Canonicalization** - Date patterns stripped from codes
4. **File-based logging** - .log and .jsonl files track everything
5. **Preview generation** - LLM sees actual columns (fixed today)
6. **Column metadata** - Descriptions persist and export (fixed today)

### What's Broken ‚ùå

1. **Database observability** - 8/10 tables empty
2. **LLM cost tracking** - No record of API calls, tokens, cost
3. **Manifest file tracking** - Can't query load history from database
4. **Error tracking** - No database queries for failed loads
5. **Reference matching** - Untested for subsequent periods
6. **Phase 1 registry** - `tbl_canonical_sources`, `tbl_source_mappings` never used

---

## Options Forward

### Option A: Complete v2.2 (Medium Effort)

**Fix database observability by adding EventStore database sink:**

```python
# src/datawarp/supervisor/events.py

def _emit_database(self, event: Event):
    """Persist events to database tables."""
    with get_connection() as conn:
        if event.event_type == EventType.LLM_CALL:
            # INSERT INTO datawarp.tbl_enrichment_api_calls
            conn.execute("""
                INSERT INTO datawarp.tbl_enrichment_api_calls
                (run_id, model_name, input_tokens, output_tokens, latency_ms)
                VALUES (%s, %s, %s, %s, %s)
            """, (event.run_id, event.model, event.input_tokens, ...))

        elif event.event_type == EventType.PERIOD_STARTED:
            # INSERT INTO datawarp.tbl_manifest_files

        elif event.event_type == EventType.STAGE_COMPLETED:
            # UPDATE status fields
```

**Pros:**
- Preserves EventStore architecture
- Maintains file + database logging
- Fixes all observability

**Cons:**
- 2-3 hours development
- Requires testing
- More code to maintain

### Option B: Revert to batch.py (Nuclear Option)

**Go back to pre-v2.2:**

```bash
git checkout b4bf5ee -- scripts/batch.py
git checkout b4bf5ee -- scripts/url_to_manifest.py
# Remove EventStore references
# Use old workflow
```

**Pros:**
- Immediate fix
- Known working state
- All observability works

**Cons:**
- Lose EventStore benefits
- Lose code organization improvements
- Feels like defeat

### Option C: Hybrid Approach (Pragmatic)

**Keep EventStore for file logging, add direct database calls:**

```python
# scripts/backfill.py

# After successful load
event_store.emit(...)  # File logging

# ALSO do direct database tracking
repository.record_manifest_file(...)  # Database logging
repository.log_enrichment_run(...)
```

**Pros:**
- Quick fix (30 minutes)
- Best of both worlds
- Low risk

**Cons:**
- Duplicate logging logic
- Not architecturally clean

### Option D: Accept Current State (Minimalist)

**Use .jsonl files as database:**

```python
# Query logs instead of database
def get_load_history(publication: str) -> List[dict]:
    events = []
    for jsonl_file in Path('logs/events').rglob('*.jsonl'):
        with open(jsonl_file) as f:
            for line in f:
                event = json.loads(line)
                if event['publication'] == publication:
                    events.append(event)
    return events
```

**Pros:**
- Zero development
- Files have all data
- Works today

**Cons:**
- Database tables are wasted
- Queries are slow
- Not scalable

---

## Testing Gaps

### What Was Never Tested

1. **Reference-based enrichment** - Subsequent periods reusing codes
2. **Cross-period consolidation** - Data from multiple periods in one table
3. **Column metadata persistence** - End-to-end from LLM to Parquet
4. **Database observability** - Query interface for load history
5. **Error recovery** - Failed loads and retries
6. **Large datasets** - 1M+ rows, 100+ columns
7. **Concurrent loads** - Multiple publications at once

### What Needs Validation

1. Run full backfill on 2+ periods of same publication
2. Verify reference matching works (codes reused)
3. Verify cross-period queries work (single table)
4. Verify column metadata exports correctly
5. Verify database tables populate (if fixed)

---

## Recommendations for Opus

### Immediate Priority (P0)

1. **Decision:** Choose Option A, B, C, or D above
2. **Fix database observability** - Complete EventStore or revert
3. **Test reference matching** - Load May + November ADHD data
4. **Validate column metadata** - End-to-end verification

### Short Term (P1)

1. **Integration tests** - Full pipeline validation
2. **Documentation** - Update for v2.2 changes
3. **Clean up dead tables** - Remove or populate
4. **Error handling** - Better failure reporting

### Long Term (P2)

1. **Performance testing** - Large datasets
2. **Concurrent loading** - Parallel processing
3. **Cost tracking** - LLM usage monitoring
4. **Query interface** - CLI commands for observability

---

## Code Comparison: Pre vs Post v2.2

### Manifest Generation

**Before (b4bf5ee^):**
```python
# scripts/url_to_manifest.py
def main(url, output_path, enable_preview=True):  # ‚Üê Default: True
    resources = scrape_resources(url)
    processed = process_resources(resources, enable_preview=True)  # ‚Üê Preview ON
    manifest = generate_manifest(processed)
    write_yaml(manifest, output_path)
```

**After (53e4988):**
```python
# src/datawarp/pipeline/manifest.py
def generate_manifest(url, output_path, skip_preview=False):  # ‚Üê Never used
    resources = scrape_resources(url)
    processed = process_resources(resources)  # ‚Üê No preview generation
    manifest = create_manifest(processed)
    write_yaml(manifest, output_path)
```

### Data Loading

**Before:**
```python
# batch.py
for file in manifest['files']:
    result = load_file(file)
    repository.record_manifest_file(manifest, file, result)  # ‚Üê DB tracking
    print(f"Loaded {result.rows}")
```

**After:**
```python
# backfill.py
for file in manifest['files']:
    result = load_file(file)
    event_store.emit(create_event(...))  # ‚Üê File tracking only
    # ‚ùå No repository.record_manifest_file()
```

---

## Files to Review

### Critical Files (Broken)
- `src/datawarp/supervisor/events.py` - EventStore missing database sink
- `scripts/backfill.py` - No database tracking calls
- `src/datawarp/pipeline/manifest.py` - Preview generation missing (fixed)

### Working Files (OK)
- `src/datawarp/core/extractor.py` - Structure detection works
- `src/datawarp/loader/pipeline.py` - Data loading works
- `src/datawarp/pipeline/canonicalize.py` - Date stripping works

### Orphaned Files (Dead Code)
- `scripts/batch.py` - Old loader, replaced but better
- `src/datawarp/loader/batch.py` - References to old workflow

---

## Conclusion

The v2.2 refactoring improved architecture but broke critical features through incomplete implementation. Three major issues discovered:

1. ‚úÖ **Preview generation** - FIXED (2026-01-13, commit de77cf9)
2. ‚úÖ **Column metadata** - FIXED (2026-01-13, commit b5785da)
3. ‚ùå **Database observability** - NOT FIXED (8/10 tables empty)

**Current status:** System works for loading data but lacks observability and error tracking. Database tables exist but aren't populated.

**Recommendation:** Implement Option A (complete EventStore) or Option C (hybrid approach) to restore database observability without reverting architecture improvements.

---

**Document prepared:** 2026-01-13 15:30 UTC
**Session context:** b4ad5740-b892-4c6a-af41-aa8e7692a5a2
**For review by:** Claude Opus 4.5
